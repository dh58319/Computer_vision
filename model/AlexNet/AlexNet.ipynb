{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define model parameters\n",
    "args = {\n",
    "    \"NUM_EPOCHS\": 90, #from original paper\n",
    "    \"BATCH_SIZE\": 128,\n",
    "    \"MOMENTUM\": 0.9,\n",
    "    \"LR_DECAY\": 0.0005,\n",
    "    \"LR_INIT\": 0.01,\n",
    "    \"IMAGE_DIM\": 227,\n",
    "    \"NUM_CLASSES\": 1000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The archive ILSVRC2012_devkit_t12.tar.gz is not present in the root directory or is corrupted. You need to download it externally and place it in data.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#import imagenet dataset\u001b[39;00m\n\u001b[1;32m      2\u001b[0m transform \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mCompose([\n\u001b[1;32m      3\u001b[0m     transforms\u001b[39m.\u001b[39mResize((args[\u001b[39m\"\u001b[39m\u001b[39mIMAGE_DIM\u001b[39m\u001b[39m\"\u001b[39m], args[\u001b[39m\"\u001b[39m\u001b[39mIMAGE_DIM\u001b[39m\u001b[39m\"\u001b[39m])),\n\u001b[1;32m      4\u001b[0m     transforms\u001b[39m.\u001b[39mToTensor()]\n\u001b[1;32m      5\u001b[0m )\n\u001b[0;32m----> 6\u001b[0m train_dataset \u001b[39m=\u001b[39m datasets\u001b[39m.\u001b[39;49mImageNet(root\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m'\u001b[39;49m, split\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m, download\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, transform\u001b[39m=\u001b[39;49mtransform)\n\u001b[1;32m      7\u001b[0m test_dataset \u001b[39m=\u001b[39m datasets\u001b[39m.\u001b[39mImageNet(root\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m, split\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m, download\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, transform\u001b[39m=\u001b[39mtransform)\n\u001b[1;32m      9\u001b[0m \u001b[39m#load data\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf/lib/python3.8/site-packages/torchvision/datasets/imagenet.py:46\u001b[0m, in \u001b[0;36mImageNet.__init__\u001b[0;34m(self, root, split, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m root \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexpanduser(root)\n\u001b[1;32m     44\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit \u001b[39m=\u001b[39m verify_str_arg(split, \u001b[39m\"\u001b[39m\u001b[39msplit\u001b[39m\u001b[39m\"\u001b[39m, (\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mval\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m---> 46\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparse_archives()\n\u001b[1;32m     47\u001b[0m wnid_to_classes \u001b[39m=\u001b[39m load_meta_file(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot)[\u001b[39m0\u001b[39m]\n\u001b[1;32m     49\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit_folder, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf/lib/python3.8/site-packages/torchvision/datasets/imagenet.py:59\u001b[0m, in \u001b[0;36mImageNet.parse_archives\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparse_archives\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m check_integrity(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot, META_FILE)):\n\u001b[0;32m---> 59\u001b[0m         parse_devkit_archive(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroot)\n\u001b[1;32m     61\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misdir(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit_folder):\n\u001b[1;32m     62\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf/lib/python3.8/site-packages/torchvision/datasets/imagenet.py:140\u001b[0m, in \u001b[0;36mparse_devkit_archive\u001b[0;34m(root, file)\u001b[0m\n\u001b[1;32m    137\u001b[0m     file \u001b[39m=\u001b[39m archive_meta[\u001b[39m0\u001b[39m]\n\u001b[1;32m    138\u001b[0m md5 \u001b[39m=\u001b[39m archive_meta[\u001b[39m1\u001b[39m]\n\u001b[0;32m--> 140\u001b[0m _verify_archive(root, file, md5)\n\u001b[1;32m    142\u001b[0m \u001b[39mwith\u001b[39;00m get_tmp_dir() \u001b[39mas\u001b[39;00m tmp_dir:\n\u001b[1;32m    143\u001b[0m     extract_archive(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(root, file), tmp_dir)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf/lib/python3.8/site-packages/torchvision/datasets/imagenet.py:96\u001b[0m, in \u001b[0;36m_verify_archive\u001b[0;34m(root, file, md5)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m check_integrity(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(root, file), md5):\n\u001b[1;32m     92\u001b[0m     msg \u001b[39m=\u001b[39m (\n\u001b[1;32m     93\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe archive \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m is not present in the root directory or is corrupted. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     94\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou need to download it externally and place it in \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     95\u001b[0m     )\n\u001b[0;32m---> 96\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg\u001b[39m.\u001b[39mformat(file, root))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The archive ILSVRC2012_devkit_t12.tar.gz is not present in the root directory or is corrupted. You need to download it externally and place it in data."
     ]
    }
   ],
   "source": [
    "#import imagenet dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((args[\"IMAGE_DIM\"], args[\"IMAGE_DIM\"])),\n",
    "    transforms.ToTensor()]\n",
    ")\n",
    "train_dataset = datasets.ImageNet(root='data', split='train', download=True, transform=transform)\n",
    "test_dataset = datasets.ImageNet(root='data', split='val', download=True, transform=transform)\n",
    "\n",
    "#load data\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=args[\"BATCH_SIZE\"], shuffle=True)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=args[\"BATCH_SIZE\"], shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 96, kernel_size=11, stride=4)\n",
    "        self.conv2 = nn.Conv2d(96, 256, kernel_size=5, padding=2)\n",
    "        self.conv3 = nn.Conv2d(256, 384, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(384, 384, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(384, 256, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(256 * 6 * 6, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        self.fc3 = nn.Linear(4096, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "        x = x.view(x.size(0), 256 * 6 * 6)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AlexNet().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=args[\"LR_INIT\"], momentum=args[\"MOMENTUM\"], weight_decay=args[\"LR_DECAY\"])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Data loading code\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:05:16) \n[Clang 12.0.1 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52ee2977380704a66854748a73250e0671a9318bd5b3fd45a3df9f851ae61629"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
